name: CICD Pipeline

# When should this workflow run?
on:
  push:
    branches:
      - main # run when code is pushed directly to main
      - "feature/*" # run when code is pushed to any feature branch
  pull_request:
    branches:
      - main # run when a PR is opened/updated against main
  workflow_dispatch: # allow a manual run from the GitHub Actions UI

jobs:
  # Job 1: Build a Docker image and push it to DockerHub (only when relevant files changed)
  build-and-push-image:
    runs-on: ubuntu-latest # run this job on a hosted Ubuntu runner
    steps:
      # Step: pulls your repository code onto the runner machine
      - name: Checkout code
        uses: actions/checkout@v4

      # Step: checks whether certain files changed in this commit/PR
      # We use this to avoid rebuilding the Docker image when nothing related changed.
      - name: Get changed files
        id: changed-files-build
        uses: tj-actions/changed-files@v45
        with:
          files: |
            Dockerfile
            requirements.txt

      # Docker Buildx enables modern builds (multi-platform, caching, and buildx build --push)
      - name: Set up Docker Buildx
        # Only do image build steps if relevant files changed OR this is a manual run
        if: steps.changed-files-build.outputs.any_changed == 'true' || github.event_name == 'workflow_dispatch'
        uses: docker/setup-buildx-action@v3

      # Login is required so `docker buildx build --push` can upload to DockerHub
      - name: Log in to DockerHub
        if: steps.changed-files-build.outputs.any_changed == 'true' || github.event_name == 'workflow_dispatch'
        uses: docker/login-action@v3
        with:
          # "vars" are repository variables (not secret); "secrets" are encrypted
          username: ${{ vars.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}

      # Build the image from the Dockerfile and push it to DockerHub
      - name: Build and push Docker image
        if: steps.changed-files-build.outputs.any_changed == 'true' || github.event_name == 'workflow_dispatch'
        run: |
          docker buildx build --push \
            --tag ${{ vars.DOCKERHUB_NAMESPACE }}/${{ vars.DOCKERHUB_REPOSITORY }}:latest \
            --tag ${{ vars.DOCKERHUB_NAMESPACE }}/${{ vars.DOCKERHUB_REPOSITORY }}:${{ github.sha }} \
            .

  # Job 2: Spin up the Airflow stack with Docker Compose and run tests inside it
  unit-and-integration-and-e2e-tests:
    runs-on: ubuntu-latest
    needs: build-and-push-image # do not start tests until the image job finishes

    # These environment variables are provided to any `run:` steps in this job.
    # In practice, docker-compose will also pick these up if your compose file references them.
    env:
      # Airflow web UI login for the test environment (so Airflow services can start cleanly)
      AIRFLOW_WWW_USER_PASSWORD: ${{ secrets.AIRFLOW_WWW_USER_PASSWORD }}
      AIRFLOW_WWW_USER_USERNAME: ${{ secrets.AIRFLOW_WWW_USER_USERNAME }}

      # Your app's secrets/config used by DAGs and tests
      API_KEY: ${{ secrets.API_KEY }}

      # Celery backend settings (only relevant if your Airflow uses CeleryExecutor)
      CELERY_BACKEND_NAME: ${{ secrets.CELERY_BACKEND_NAME }}
      CELERY_BACKEND_PASSWORD: ${{ secrets.CELERY_BACKEND_PASSWORD }}
      CELERY_BACKEND_USERNAME: ${{ secrets.CELERY_BACKEND_USERNAME }}

      # DB used by your ELT pipeline code/tests
      ELT_DATABASE_NAME: ${{ secrets.ELT_DATABASE_NAME }}
      ELT_DATABASE_PASSWORD: ${{ secrets.ELT_DATABASE_PASSWORD }}
      ELT_DATABASE_USERNAME: ${{ secrets.ELT_DATABASE_USERNAME }}

      # Airflow uses a Fernet key to encrypt/decrypt connections/variables in metadata DB
      FERNET_KEY: ${{ secrets.FERNET_KEY }}

      # Airflow metadata database credentials (Airflow stores DAG runs, task instances, connections, etc.)
      METADATA_DATABASE_NAME: ${{ secrets.METADATA_DATABASE_NAME }}
      METADATA_DATABASE_PASSWORD: ${{ secrets.METADATA_DATABASE_PASSWORD }}
      METADATA_DATABASE_USERNAME: ${{ secrets.METADATA_DATABASE_USERNAME }}

      # Postgres service config used by docker-compose (and possibly tests)
      POSTGRES_USER: ${{ secrets.POSTGRES_CONN_USERNAME }}
      POSTGRES_PASSWORD: ${{ secrets.POSTGRES_CONN_PASSWORD }}

      # Connection data the code/tests read to connect to Postgres
      POSTGRES_CONN_HOST: ${{ secrets.POSTGRES_CONN_HOST }}
      POSTGRES_CONN_PASSWORD: ${{ secrets.POSTGRES_CONN_PASSWORD }}
      POSTGRES_CONN_PORT: ${{ secrets.POSTGRES_CONN_PORT }}
      POSTGRES_CONN_USERNAME: ${{ secrets.POSTGRES_CONN_USERNAME }}

      # Non-secret configuration values stored as repo variables
      AIRFLOW_UID: ${{ vars.AIRFLOW_UID }} # file ownership mapping for containers (Linux UID)
      CHANNEL_HANDLE: ${{ vars.CHANNEL_HANDLE }} # your default YouTube channel handle
      DOCKERHUB_NAMESPACE: ${{ vars.DOCKERHUB_NAMESPACE }}
      DOCKERHUB_REPOSITORY: ${{ vars.DOCKERHUB_REPOSITORY }}
      DOCKERHUB_USERNAME: ${{ vars.DOCKERHUB_USERNAME }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # Same trick as above: only run expensive tests if certain folders/files changed
      - name: Get changed files
        id: changed-files-tests
        uses: tj-actions/changed-files@v45
        with:
          files: |
            dags/**
            include/**
            docker-compose.yaml

      # Bring up your whole local stack (Airflow + Postgres + whatever is in compose)
      - name: Set up Docker Compose
        if: steps.changed-files-tests.outputs.any_changed == 'true' || github.event_name == 'workflow_dispatch'
        run: |
          # -d means "detached" (run containers in the background on the runner)
          docker compose up -d

      # Unit/integration tests: run pytest inside the running Airflow worker container
      - name: Run Unit and Integration Tests
        if: steps.changed-files-tests.outputs.any_changed == 'true' || github.event_name == 'workflow_dispatch'
        run: |
          docker exec -t airflow-worker sh -c "pytest tests/ -v"

      # End-to-end tests: Airflow's built-in "dags test" runs a DAG locally for a single execution date
      # This is closer to "does the pipeline actually run" than unit tests.
      - name: Run End-to-End DAG Tests
        if: steps.changed-files-tests.outputs.any_changed == 'true' || github.event_name == 'workflow_dispatch'
        run: |
          DAG_NAMES=("produce_json" "update_db" "data_quality")
          for DAG in "${DAG_NAMES[@]}"; do
            docker exec -t airflow-worker sh -c "airflow dags test $DAG"
          done

      # Always shut everything down so the GitHub runner is left clean
      - name: Tear down Docker Compose
        if: steps.changed-files-tests.outputs.any_changed == 'true' || github.event_name == 'workflow_dispatch'
        run: |
          docker compose down
